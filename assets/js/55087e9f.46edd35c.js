"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5500],{758:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"NLP/transformer","title":"Transformer","description":"based on attention is all you need","source":"@site/docs/NLP/transformer.md","sourceDirName":"NLP","slug":"/NLP/transformer","permalink":"/docs/NLP/transformer","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"speech","permalink":"/docs/NLP/speech"},"next":{"title":"MAT","permalink":"/docs/category/mat"}}');var i=t(4848),s=t(8453);const l={},o="Transformer",a={},d=[{value:"Attention",id:"attention",level:2},{value:"Encode",id:"encode",level:2},{value:"Decode",id:"decode",level:2},{value:"tansformer based language model",id:"tansformer-based-language-model",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"transformer",children:"Transformer"})}),"\n",(0,i.jsx)(n.p,{children:"based on attention is all you need"}),"\n",(0,i.jsx)(n.h2,{id:"attention",children:"Attention"}),"\n",(0,i.jsxs)(n.p,{children:["attention(Q,K,V): Q",":query","; K: key; V: value"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Attention(Q,K,V) = softmax(QK.T/sqrt(dK))V"}),"\n",(0,i.jsx)(n.li,{children:"dK stand for dimension of matrix K"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"encode",children:"Encode"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Residual connection","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"some problems here! NN struglle to learn the identity function mapping"}),"\n",(0,i.jsx)(n.li,{children:"solution: add back the input embeddings to the sub-layer's output moving up"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Layer Normalization","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"what we do: Normlaize input layer's distribution to 0 mean and 1 standard deviation"}),"\n",(0,i.jsx)(n.li,{children:"why we should do? Removes uninformative variation in layer's features"}),"\n",(0,i.jsx)(n.li,{children:"some problems here! we call this post-LN, some times we need warm up with learn rate."}),"\n",(0,i.jsx)(n.li,{children:"alternative: pre-LN: puts layer-norm within the residual block. Allows removing warm-up stage. More stable training initialization."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Transformer Encoder - Self Attention, steps:","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"calculate the quer, key and value for each toekn"}),"\n",(0,i.jsx)(n.li,{children:"calculate the attention score between query and keys"}),"\n",(0,i.jsx)(n.li,{children:"normalize the attention scoress by applying softmax"}),"\n",(0,i.jsx)(n.li,{children:"calculate values by taking a weighted sum"}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"we also have Multi-head self Attention (MHA)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"feed-forward(FFN) layers:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"why we need? Attention only re-weight the value vectors, we need to apply non-linearities (activations_ to enable (deep) learning"}),"\n",(0,i.jsx)(n.li,{children:"FFn provide non-linear activation to attention layer outputs (what we want)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"position encoding:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"why we need? attention"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"decode",children:"Decode"}),"\n",(0,i.jsx)(n.p,{children:"Masked self-attention:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"enforce auto-regressive language modeling objective. the decoder cannot peek and pay attention to the unknown future words"}),"\n",(0,i.jsx)(n.li,{children:"solution: use a look-head mask M, by setting attention"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"tansformer-based-language-model",children:"tansformer based language model"}),"\n",(0,i.jsx)(n.p,{children:"BERT; bidirectional Encoder Representations from Transformers"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"impressive performance, performance better than human and SOTA peers"}),"\n",(0,i.jsx)(n.li,{children:"encoder-only architecturue, lead some problem like:"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"BART; adds the decoder back to BerT and keepign the BERT objective"}),"\n",(0,i.jsx)(n.p,{children:"NMT"}),"\n",(0,i.jsx)(n.p,{children:"T5: Text-to-Text Transfer Transformer"}),"\n",(0,i.jsx)(n.p,{children:"GPT: generative Pretrained Transformer"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"GPT3: very alrge models (175B parameters)"}),"\n",(0,i.jsx)(n.li,{children:"prompt does matter"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function l(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);