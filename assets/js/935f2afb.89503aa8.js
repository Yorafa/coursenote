"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[53],{4612:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Intro","href":"/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Basic","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Function","href":"/docs/basics/Function","docId":"basics/Function","unlisted":false},{"type":"link","label":"GCD && LCM","href":"/docs/basics/GCD && LCM","docId":"basics/GCD && LCM","unlisted":false},{"type":"link","label":"Matrix","href":"/docs/basics/Matrix","docId":"basics/Matrix","unlisted":false},{"type":"link","label":"Modular Arithmetic","href":"/docs/basics/Modular Arithmetic","docId":"basics/Modular Arithmetic","unlisted":false},{"type":"link","label":"Sequence","href":"/docs/basics/Sequence","docId":"basics/Sequence","unlisted":false},{"type":"link","label":"Set","href":"/docs/basics/Set","docId":"basics/Set","unlisted":false},{"type":"link","label":"Some Basic Discrete Math Definition","href":"/docs/basics/Some Basic Discrete Math Definition","docId":"basics/Some Basic Discrete Math Definition","unlisted":false},{"type":"link","label":"Some Common Series Used","href":"/docs/basics/Some Common Series Used","docId":"basics/Some Common Series Used","unlisted":false},{"type":"link","label":"Some Short Form English","href":"/docs/basics/Some Short Form English","docId":"basics/Some Short Form English","unlisted":false},{"type":"link","label":"Vector","href":"/docs/basics/Vector","docId":"basics/Vector","unlisted":false},{"type":"link","label":"Some Basic Knowledge","href":"/docs/basics/","docId":"basics/basics","unlisted":false}],"href":"/docs/category/basic"},{"type":"category","label":"CSC","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"CSC209 Software Tools and Systems Programming","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Bit Manipulation","href":"/docs/csc/csc209/Bit","docId":"csc/csc209/Bit","unlisted":false},{"type":"link","label":"IO","href":"/docs/csc/csc209/IO","docId":"csc/csc209/IO","unlisted":false},{"type":"link","label":"Makefiles","href":"/docs/csc/csc209/Makefiles","docId":"csc/csc209/Makefiles","unlisted":false},{"type":"link","label":"Pipe","href":"/docs/csc/csc209/Pipe","docId":"csc/csc209/Pipe","unlisted":false},{"type":"link","label":"Pointer and Memory","href":"/docs/csc/csc209/Pointer & Memory","docId":"csc/csc209/Pointer & Memory","unlisted":false},{"type":"link","label":"Process Model","href":"/docs/csc/csc209/ProcessModel","docId":"csc/csc209/ProcessModel","unlisted":false},{"type":"link","label":"Read & Write in File","href":"/docs/csc/csc209/Read&Write in file","docId":"csc/csc209/Read&Write in file","unlisted":false},{"type":"link","label":"Select system call","href":"/docs/csc/csc209/Select","docId":"csc/csc209/Select","unlisted":false},{"type":"link","label":"Shell Programming","href":"/docs/csc/csc209/Shell","docId":"csc/csc209/Shell","unlisted":false},{"type":"link","label":"Signal","href":"/docs/csc/csc209/Signal","docId":"csc/csc209/Signal","unlisted":false},{"type":"link","label":"Socket","href":"/docs/csc/csc209/Socket","docId":"csc/csc209/Socket","unlisted":false},{"type":"link","label":"String","href":"/docs/csc/csc209/String","docId":"csc/csc209/String","unlisted":false}],"href":"/docs/csc/csc209/"},{"type":"category","label":"CSC236 Introduction to the Theory of Computation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Binary Relations","href":"/docs/csc/csc236/Binary Relations","docId":"csc/csc236/Binary Relations","unlisted":false},{"type":"link","label":"Correctness","href":"/docs/csc/csc236/Correctness","docId":"csc/csc236/Correctness","unlisted":false},{"type":"link","label":"Finite Automata","href":"/docs/csc/csc236/Finite Automata","docId":"csc/csc236/Finite Automata","unlisted":false},{"type":"link","label":"Graph","href":"/docs/csc/csc236/Graph","docId":"csc/csc236/Graph","unlisted":false},{"type":"link","label":"Induction","href":"/docs/csc/csc236/Induction","docId":"csc/csc236/Induction","unlisted":false},{"type":"link","label":"Recurrences","href":"/docs/csc/csc236/Recurrences","docId":"csc/csc236/Recurrences","unlisted":false}],"href":"/docs/csc/csc236/"},{"type":"category","label":"CSC263 Data Structures and Analysis","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"AVL Tree","href":"/docs/csc/csc263/AVL Tree","docId":"csc/csc263/AVL Tree","unlisted":false},{"type":"link","label":"Amortized Analysis","href":"/docs/csc/csc263/Amortized Analysis","docId":"csc/csc263/Amortized Analysis","unlisted":false},{"type":"link","label":"Binary Counter","href":"/docs/csc/csc263/Binary Counter","docId":"csc/csc263/Binary Counter","unlisted":false},{"type":"link","label":"Binary Search Tree","href":"/docs/csc/csc263/Binary Search Tree","docId":"csc/csc263/Binary Search Tree","unlisted":false},{"type":"link","label":"Binomial Heap","href":"/docs/csc/csc263/Binomial Heap","docId":"csc/csc263/Binomial Heap","unlisted":false},{"type":"link","label":"Dictionary","href":"/docs/csc/csc263/Dictionary","docId":"csc/csc263/Dictionary","unlisted":false},{"type":"link","label":"Disjoint Set","href":"/docs/csc/csc263/Disjoint Set","docId":"csc/csc263/Disjoint Set","unlisted":false},{"type":"link","label":"Dynamic Array","href":"/docs/csc/csc263/Dynamic Array","docId":"csc/csc263/Dynamic Array","unlisted":false},{"type":"link","label":"Graph","href":"/docs/csc/csc263/Graph","docId":"csc/csc263/Graph","unlisted":false},{"type":"link","label":"Hash","href":"/docs/csc/csc263/Hash","docId":"csc/csc263/Hash","unlisted":false},{"type":"link","label":"Heap","href":"/docs/csc/csc263/Heap","docId":"csc/csc263/Heap","unlisted":false},{"type":"link","label":"Minimum Spanning Tree","href":"/docs/csc/csc263/Minimum Spanning Tree","docId":"csc/csc263/Minimum Spanning Tree","unlisted":false},{"type":"link","label":"Ordered Set","href":"/docs/csc/csc263/Ordered Set","docId":"csc/csc263/Ordered Set","unlisted":false},{"type":"link","label":"Priority Queue","href":"/docs/csc/csc263/Priority Queue","docId":"csc/csc263/Priority Queue","unlisted":false},{"type":"link","label":"Quick Sort","href":"/docs/csc/csc263/Quick Sort","docId":"csc/csc263/Quick Sort","unlisted":false}],"href":"/docs/csc/csc263/"},{"type":"category","label":"CSC369: Operating Systems","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OS - three piece","href":"/docs/csc/csc369/00 intro","docId":"csc/csc369/00 intro","unlisted":false},{"type":"link","label":"Process","href":"/docs/csc/csc369/01 process","docId":"csc/csc369/01 process","unlisted":false},{"type":"link","label":"System Call","href":"/docs/csc/csc369/02 system call","docId":"csc/csc369/02 system call","unlisted":false},{"type":"link","label":"Thread","href":"/docs/csc/csc369/03 thread","docId":"csc/csc369/03 thread","unlisted":false},{"type":"link","label":"Synchronization","href":"/docs/csc/csc369/04 Synchronization","docId":"csc/csc369/04 Synchronization","unlisted":false},{"type":"link","label":"Scheduling","href":"/docs/csc/csc369/05 Scheduling","docId":"csc/csc369/05 Scheduling","unlisted":false},{"type":"link","label":"Memory Management","href":"/docs/csc/csc369/06 Memory Management","docId":"csc/csc369/06 Memory Management","unlisted":false},{"type":"link","label":"Paging and Address Translation","href":"/docs/csc/csc369/07 Paging and Address Translation","docId":"csc/csc369/07 Paging and Address Translation","unlisted":false},{"type":"link","label":"Principles and Policies for Memory Management","href":"/docs/csc/csc369/08 Principles and Policies for Memory Management","docId":"csc/csc369/08 Principles and Policies for Memory Management","unlisted":false},{"type":"link","label":"File System","href":"/docs/csc/csc369/09 file system introduction","docId":"csc/csc369/09 file system introduction","unlisted":false},{"type":"link","label":"File System Reliability","href":"/docs/csc/csc369/10 file system reliability","docId":"csc/csc369/10 file system reliability","unlisted":false}],"href":"/docs/csc/csc369/"},{"type":"category","label":"CSC413 Neural Networks and Deep Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Backpropagation","href":"/docs/csc/csc413/Backpropagation","docId":"csc/csc413/Backpropagation","unlisted":false},{"type":"link","label":"CNN and Image Classification","href":"/docs/csc/csc413/CNN","docId":"csc/csc413/CNN","unlisted":false},{"type":"link","label":"Intro to NN","href":"/docs/csc/csc413/Intro to NN","docId":"csc/csc413/Intro to NN","unlisted":false},{"type":"link","label":"Language Model","href":"/docs/csc/csc413/Language Model","docId":"csc/csc413/Language Model","unlisted":false},{"type":"link","label":"Multi-Layer Perceptrons","href":"/docs/csc/csc413/Multilayer Perceptrons","docId":"csc/csc413/Multilayer Perceptrons","unlisted":false},{"type":"link","label":"Optimization","href":"/docs/csc/csc413/Optimization","docId":"csc/csc413/Optimization","unlisted":false}],"href":"/docs/csc/csc413/"}],"href":"/docs/category/csc"},{"type":"category","label":"MAT","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"MAT301 Groups and Symmetries","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Equivalence Relations","href":"/docs/mat/mat301/0 Equivalence Relations","docId":"mat/mat301/0 Equivalence Relations","unlisted":false},{"type":"link","label":"Group Definition","href":"/docs/mat/mat301/1 Group Definition","docId":"mat/mat301/1 Group Definition","unlisted":false},{"type":"link","label":"Cyclic Group","href":"/docs/mat/mat301/2 Cyclic Group","docId":"mat/mat301/2 Cyclic Group","unlisted":false},{"type":"link","label":"Permutation Group","href":"/docs/mat/mat301/3 Permutation Group","docId":"mat/mat301/3 Permutation Group","unlisted":false},{"type":"link","label":"Group Isomorphisms","href":"/docs/mat/mat301/4 Isomorphisms","docId":"mat/mat301/4 Isomorphisms","unlisted":false},{"type":"link","label":"Cosets","href":"/docs/mat/mat301/5 Cosets","docId":"mat/mat301/5 Cosets","unlisted":false},{"type":"link","label":"Direct Products","href":"/docs/mat/mat301/6 Direct Products","docId":"mat/mat301/6 Direct Products","unlisted":false},{"type":"link","label":"Normal subgroups","href":"/docs/mat/mat301/6 Normal subgroups","docId":"mat/mat301/6 Normal subgroups","unlisted":false},{"type":"link","label":"Group Homomorphisms","href":"/docs/mat/mat301/7 Homomorphisms","docId":"mat/mat301/7 Homomorphisms","unlisted":false},{"type":"link","label":"Fundamental Theorem of Finite Abelian Groups","href":"/docs/mat/mat301/8 Fundamental Theorem of Finite Abelian Groups","docId":"mat/mat301/8 Fundamental Theorem of Finite Abelian Groups","unlisted":false},{"type":"link","label":"Sylow Theorems","href":"/docs/mat/mat301/8 Sylow Theorem","docId":"mat/mat301/8 Sylow Theorem","unlisted":false},{"type":"link","label":"Group Action","href":"/docs/mat/mat301/Group Action","docId":"mat/mat301/Group Action","unlisted":false},{"type":"link","label":"More Groups","href":"/docs/mat/mat301/Group Class","docId":"mat/mat301/Group Class","unlisted":false}],"href":"/docs/mat/mat301/"},{"type":"category","label":"MAT337 Introduction to Real Analysis","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Differential equations and integral calculus","href":"/docs/mat/mat337/Differential equations and integral calculus","docId":"mat/mat337/Differential equations and integral calculus","unlisted":false},{"type":"link","label":"Functions","href":"/docs/mat/mat337/Function","docId":"mat/mat337/Function","unlisted":false},{"type":"link","label":"Introduction to Real Number","href":"/docs/mat/mat337/Introduction to Real Number","docId":"mat/mat337/Introduction to Real Number","unlisted":false},{"type":"link","label":"Limits of Functions","href":"/docs/mat/mat337/Limits of Functions","docId":"mat/mat337/Limits of Functions","unlisted":false},{"type":"link","label":"Metric Spaces","href":"/docs/mat/mat337/Metric Spaces","docId":"mat/mat337/Metric Spaces","unlisted":false},{"type":"link","label":"Norms","href":"/docs/mat/mat337/Norms","docId":"mat/mat337/Norms","unlisted":false},{"type":"link","label":"Sequence","href":"/docs/mat/mat337/Sequence","docId":"mat/mat337/Sequence","unlisted":false},{"type":"link","label":"Series","href":"/docs/mat/mat337/Series","docId":"mat/mat337/Series","unlisted":false},{"type":"link","label":"Topology Of N Demensional Real Number","href":"/docs/mat/mat337/Topology Of N Demensional Real Number","docId":"mat/mat337/Topology Of N Demensional Real Number","unlisted":false}],"href":"/docs/mat/mat337/"}],"href":"/docs/category/mat"},{"type":"category","label":"STA","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"STA314 Introduction to Machine/statisitcal Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Parametric V.S. Non-parametric","href":"/docs/sta/sta314/Basic ML info","docId":"sta/sta314/Basic ML info","unlisted":false},{"type":"link","label":"Moving beyond Linearity","href":"/docs/sta/sta314/Beyond Linearity","docId":"sta/sta314/Beyond Linearity","unlisted":false},{"type":"link","label":"Classification","href":"/docs/sta/sta314/Classfication","docId":"sta/sta314/Classfication","unlisted":false},{"type":"link","label":"Decision Tree","href":"/docs/sta/sta314/Decision Tree","docId":"sta/sta314/Decision Tree","unlisted":false},{"type":"link","label":"Discriminant Analysis","href":"/docs/sta/sta314/Discriminant Analysis","docId":"sta/sta314/Discriminant Analysis","unlisted":false},{"type":"link","label":"Fitted Model Measurement","href":"/docs/sta/sta314/Fitted Model Measurement","docId":"sta/sta314/Fitted Model Measurement","unlisted":false},{"type":"link","label":"Gradient Descent","href":"/docs/sta/sta314/Gradient Descent","docId":"sta/sta314/Gradient Descent","unlisted":false},{"type":"link","label":"Lasso Regression","href":"/docs/sta/sta314/Lasso Regression","docId":"sta/sta314/Lasso Regression","unlisted":false},{"type":"link","label":"Linear Model","href":"/docs/sta/sta314/Linear Model","docId":"sta/sta314/Linear Model","unlisted":false},{"type":"link","label":"Logistic Regression","href":"/docs/sta/sta314/Logistic Regression","docId":"sta/sta314/Logistic Regression","unlisted":false},{"type":"link","label":"Ordinary Least Square and Model Selection","href":"/docs/sta/sta314/Ordinary Least Square and Model Selection","docId":"sta/sta314/Ordinary Least Square and Model Selection","unlisted":false},{"type":"link","label":"Ridge Regression","href":"/docs/sta/sta314/Ridge Regression","docId":"sta/sta314/Ridge Regression","unlisted":false},{"type":"link","label":"Situation without the Test Data to do Model Validation","href":"/docs/sta/sta314/Situation without the Test Data to do Model Validation","docId":"sta/sta314/Situation without the Test Data to do Model Validation","unlisted":false},{"type":"link","label":"Support Vector Machine","href":"/docs/sta/sta314/Support Vector Machine","docId":"sta/sta314/Support Vector Machine","unlisted":false},{"type":"link","label":"Tree Improving","href":"/docs/sta/sta314/Tree Improving","docId":"sta/sta314/Tree Improving","unlisted":false},{"type":"link","label":"Some Unsupervised Learning Model","href":"/docs/sta/sta314/Unsupervised Model","docId":"sta/sta314/Unsupervised Model","unlisted":false}],"href":"/docs/sta/sta314/"},{"type":"category","label":"STA347 Probability","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Basic Properties of Probabilities","href":"/docs/sta/sta347/0 Introduction to Probabilities","docId":"sta/sta347/0 Introduction to Probabilities","unlisted":false},{"type":"link","label":"Random Variable","href":"/docs/sta/sta347/1 Random Variable","docId":"sta/sta347/1 Random Variable","unlisted":false},{"type":"link","label":"Expected Values","href":"/docs/sta/sta347/2 Expected Values, Variance and Momentum","docId":"sta/sta347/2 Expected Values, Variance and Momentum","unlisted":false},{"type":"link","label":"Random Variables Convergence","href":"/docs/sta/sta347/3 Random Variables Convergence","docId":"sta/sta347/3 Random Variables Convergence","unlisted":false},{"type":"link","label":"Change of Variable","href":"/docs/sta/sta347/4 Change of Variable","docId":"sta/sta347/4 Change of Variable","unlisted":false},{"type":"link","label":"Distributions","href":"/docs/sta/sta347/4 Distribution","docId":"sta/sta347/4 Distribution","unlisted":false},{"type":"link","label":"Inequality","href":"/docs/sta/sta347/4 Inequality","docId":"sta/sta347/4 Inequality","unlisted":false},{"type":"link","label":"Stochastic Process","href":"/docs/sta/sta347/5 Stochastic Process","docId":"sta/sta347/5 Stochastic Process","unlisted":false},{"type":"link","label":"Markov Chain","href":"/docs/sta/sta347/6 Markov Chain","docId":"sta/sta347/6 Markov Chain","unlisted":false},{"type":"link","label":"Monte Carlo Approximation","href":"/docs/sta/sta347/7 Monte Carlo Approximation","docId":"sta/sta347/7 Monte Carlo Approximation","unlisted":false},{"type":"link","label":"Review","href":"/docs/sta/sta347/Recap","docId":"sta/sta347/Recap","unlisted":false}],"href":"/docs/sta/sta347/"},{"type":"category","label":"STA414 Statistical Methods for Machine Learning II","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Conditional Independence and Bayes Nets","href":"/docs/sta/sta414/0 Bayes Nets","docId":"sta/sta414/0 Bayes Nets","unlisted":false},{"type":"link","label":"Markov Random Fields","href":"/docs/sta/sta414/1 Markov Random Fields","docId":"sta/sta414/1 Markov Random Fields","unlisted":false},{"type":"link","label":"Probabilistic Graphical Models","href":"/docs/sta/sta414/2 Probabilistic Graphical Models","docId":"sta/sta414/2 Probabilistic Graphical Models","unlisted":false},{"type":"link","label":"Sampling","href":"/docs/sta/sta414/3 Sampling","docId":"sta/sta414/3 Sampling","unlisted":false},{"type":"link","label":"Hidden Markov Model","href":"/docs/sta/sta414/4 Hidden Markov Model","docId":"sta/sta414/4 Hidden Markov Model","unlisted":false},{"type":"link","label":"Variational Inference","href":"/docs/sta/sta414/5 Variational Inference","docId":"sta/sta414/5 Variational Inference","unlisted":false},{"type":"link","label":"Mixture of Gaussians (or Gaussian Mixture Model (GMM))","href":"/docs/sta/sta414/6 Gaussians Mixture Model","docId":"sta/sta414/6 Gaussians Mixture Model","unlisted":false},{"type":"link","label":"Probabilistic Principal Component Analysis","href":"/docs/sta/sta414/7 Probabilistic PCA","docId":"sta/sta414/7 Probabilistic PCA","unlisted":false},{"type":"link","label":"Bayesian Linear Regression","href":"/docs/sta/sta414/8 Bayesian Linear Regression","docId":"sta/sta414/8 Bayesian Linear Regression","unlisted":false},{"type":"link","label":"Kernal Method","href":"/docs/sta/sta414/9 Kernal Method","docId":"sta/sta414/9 Kernal Method","unlisted":false},{"type":"link","label":"Some recap of previous courses","href":"/docs/sta/sta414/Recap","docId":"sta/sta414/Recap","unlisted":false}],"href":"/docs/sta/sta414/"},{"type":"category","label":"STA437 Method for Multivariate Data Analysis","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Basic Information to Multivariate Data","href":"/docs/sta/sta437/Basic Information to Multivariate Data","docId":"sta/sta437/Basic Information to Multivariate Data","unlisted":false},{"type":"link","label":"Multivariate Data Analysis among Machine Learning","href":"/docs/sta/sta437/Multivariate Data Analysis among Machine Learning","docId":"sta/sta437/Multivariate Data Analysis among Machine Learning","unlisted":false},{"type":"link","label":"Multivariate Normal Distribution","href":"/docs/sta/sta437/Multivariate Normal Distribution","docId":"sta/sta437/Multivariate Normal Distribution","unlisted":false}],"href":"/docs/sta/sta437/"},{"type":"category","label":"STA447 Stochastic Processe","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Markov Chain","href":"/docs/sta/sta447/0 Markov Chain","docId":"sta/sta447/0 Markov Chain","unlisted":false},{"type":"link","label":"Simple Random Walk","href":"/docs/sta/sta447/0.5 Simple Random Walk","docId":"sta/sta447/0.5 Simple Random Walk","unlisted":false},{"type":"link","label":"Markov Chain Behavior","href":"/docs/sta/sta447/1 Markov Chain Behavior","docId":"sta/sta447/1 Markov Chain Behavior","unlisted":false},{"type":"link","label":"Markov Chain Monte Carlo","href":"/docs/sta/sta447/2 Markov Chain Monte Carlo","docId":"sta/sta447/2 Markov Chain Monte Carlo","unlisted":false},{"type":"link","label":"Martingales","href":"/docs/sta/sta447/3 Martingales","docId":"sta/sta447/3 Martingales","unlisted":false},{"type":"link","label":"Continuous Processes","href":"/docs/sta/sta447/4 Continuous Processes","docId":"sta/sta447/4 Continuous Processes","unlisted":false}],"href":"/docs/sta/sta447/"}],"href":"/docs/category/sta"}]},"docs":{"basics/basics":{"id":"basics/basics","title":"Some Basic Knowledge","description":"We may find some knowledge are common and very basic, I may organize to here :)","sidebar":"tutorialSidebar"},"basics/Function":{"id":"basics/Function","title":"Function","description":"We say the way transfer an element of a collection to the other collection as function. In mathematic, we always use function to transfer an element of a set to the other set. (Obviously set is a kind of collection.) Or sample space in statistics, or parameters in CS.","sidebar":"tutorialSidebar"},"basics/GCD && LCM":{"id":"basics/GCD && LCM","title":"GCD && LCM","description":"GCD","sidebar":"tutorialSidebar"},"basics/Matrix":{"id":"basics/Matrix","title":"Matrix","description":"Matrix is the extension of vector where from $n\\\\times 1 \\\\to n\\\\times m$, but notice, the addition of matrix only perform when they are all the same $n\\\\times m$ where they have the same number of rows and columns. For multiplication, we can only perform $A{n\\\\times p} = B{n\\\\times k} \\\\cdot C_{k\\\\times p}$","sidebar":"tutorialSidebar"},"basics/Modular Arithmetic":{"id":"basics/Modular Arithmetic","title":"Modular Arithmetic","description":"let m be a fixed natural number greater than 1.  we say b modulo m if a - b is divisible by m. $a\\\\equiv b\\\\pmod  m$","sidebar":"tutorialSidebar"},"basics/Sequence":{"id":"basics/Sequence","title":"Sequence","description":"$\\\\forall n\\\\in \\\\mathbb$ we define the list number $\\\\dots,xn,x{n+1},\\\\dots$  then $\\\\{xn\\\\}$ is a sequence, or $(xn)$; some time we have multiple variable and we may have something like $(x{a,b}){a\\\\in N}$ means this sequence focus on the changing of $a$","sidebar":"tutorialSidebar"},"basics/Set":{"id":"basics/Set","title":"Set","description":"Let $C$ be a collection.  $\\\\forall a\\\\in C, \\\\nexists b\\\\in C\\\\backslash \\\\{a\\\\}, a = b \\\\implies C$ is a set.","sidebar":"tutorialSidebar"},"basics/Some Basic Discrete Math Definition":{"id":"basics/Some Basic Discrete Math Definition","title":"Some Basic Discrete Math Definition","description":"A variable is a logical variable like $x$,$y$, $p$, $q$.","sidebar":"tutorialSidebar"},"basics/Some Common Series Used":{"id":"basics/Some Common Series Used","title":"Some Common Series Used","description":"Arithmetic Progression:: $\\\\sum\\\\{i=k}^m i = (k + m)*(m - k)/2 = (m^2 - k^2)/2$;  More specific, for all $i, \\\\|a\\\\ - a\\\\{i}\\\\| = d, d\\\\in \\\\R$  we have $(a\\\\0 + a\\\\_n)*(n+1)/2$","sidebar":"tutorialSidebar"},"basics/Some Short Form English":{"id":"basics/Some Short Form English","title":"Some Short Form English","description":"Formal","sidebar":"tutorialSidebar"},"basics/Vector":{"id":"basics/Vector","title":"Vector","description":"we first define vector which is a collection of data in $n\\\\times1$ as $\\\\beginx1\\\\\\\\ x2\\\\\\\\x3\\\\\\\\\\\\vdots\\\\\\\\xn\\\\end{bmatrix}$, and matrix is a collection of data in $n\\\\times p$ as $\\\\begin{bmatrix}x{11} & x{12} & \\\\cdots & x{1p} \\\\\\\\ x{21} & x{22} & \\\\cdots & x{2p} \\\\\\\\ \\\\cdots & \\\\cdots & \\\\cdots & \\\\cdots \\\\\\\\x{n1} & x{n2} & \\\\cdots & x_{np}\\\\end{bmatrix}$","sidebar":"tutorialSidebar"},"csc/csc209/Bit":{"id":"csc/csc209/Bit","title":"Bit Manipulation","description":"Bit manipulation will manipulate numbers in binary by different  operations","sidebar":"tutorialSidebar"},"csc/csc209/csc209":{"id":"csc/csc209/csc209","title":"CSC209 Software Tools and Systems Programming","description":"Instructor: Karen Reid","sidebar":"tutorialSidebar"},"csc/csc209/IO":{"id":"csc/csc209/IO","title":"IO","description":"The \\"IO\\" standard input and output, and we can easy input/output our data in format by function followedf","sidebar":"tutorialSidebar"},"csc/csc209/Makefiles":{"id":"csc/csc209/Makefiles","title":"Makefiles","description":"Normally, we create a Makefile to make which aim to make life easier.","sidebar":"tutorialSidebar"},"csc/csc209/Pipe":{"id":"csc/csc209/Pipe","title":"Pipe","description":"Lower IO","sidebar":"tutorialSidebar"},"csc/csc209/Pointer & Memory":{"id":"csc/csc209/Pointer & Memory","title":"Pointer and Memory","description":"Pointer","sidebar":"tutorialSidebar"},"csc/csc209/ProcessModel":{"id":"csc/csc209/ProcessModel","title":"Process Model","description":"We can design program that run other program. We should first know:","sidebar":"tutorialSidebar"},"csc/csc209/Read&Write in file":{"id":"csc/csc209/Read&Write in file","title":"Read & Write in File","description":"\u505a\u670d\u52a1\u7aef\u80af\u5b9a\u9700\u8981\u4e0e\u6587\u4ef6\u8fdb\u884c\u4ea4\u4e92\u3002\u90a3\u4e48C Language \u662f\u600e\u4e48\u901a\u8fc7\u6807\u51c6\u5e93\u8bfb\u672c\u5730\u6587\u4ef6\u7684\uff0c\u53c8\u662f\u600e\u4e48\u5199\u5165\u672c\u5730\u6587\u4ef6\u7684\u5462\uff1f\u4e00\u822c\u7684\u8ba1\u7b97\u673a\u6587\u4ef6\u5206\u4e3a\u4e8c\u8fdb\u5236\u6587\u4ef6\u548cASCII\u6587\u4ef6\u4e5f\u53eb\u505a\u7eaf\u6587\u672c\u6587\u4ef6\u3002","sidebar":"tutorialSidebar"},"csc/csc209/Select":{"id":"csc/csc209/Select","title":"Select system call","description":"Select is a very useful system call when we use in pipe/socket which help us a lot in simultaneously working.","sidebar":"tutorialSidebar"},"csc/csc209/Shell":{"id":"csc/csc209/Shell","title":"Shell Programming","description":"Shell is a program that infinite looped to execute and interpreter command.","sidebar":"tutorialSidebar"},"csc/csc209/Signal":{"id":"csc/csc209/Signal","title":"Signal","description":"Segementation Fault Warning!!!!!! (not really lol)","sidebar":"tutorialSidebar"},"csc/csc209/Socket":{"id":"csc/csc209/Socket","title":"Socket","description":"Before we talk about socket, we should probably know how the internet\'s workflow.","sidebar":"tutorialSidebar"},"csc/csc209/String":{"id":"csc/csc209/String","title":"String","description":"Particular in C, there is no string type, we use char array following \\\\0 to represent a string or more officially, every string (string literal included) has a explicit \\\\0 at the end. If we use pointer to present, then is a string literal since it call the global value and we  can not do editing for those globals.","sidebar":"tutorialSidebar"},"csc/csc236/Binary Relations":{"id":"csc/csc236/Binary Relations","title":"Binary Relations","description":"Binary Relations: Denote a relation over a set $A$ be $R$, if the relation is binary relation, then $\\\\forall a,b \\\\in A$","sidebar":"tutorialSidebar"},"csc/csc236/Correctness":{"id":"csc/csc236/Correctness","title":"Correctness","description":"When we have a function, how can we show the function we write is correct?","sidebar":"tutorialSidebar"},"csc/csc236/csc236":{"id":"csc/csc236/csc236","title":"CSC236 Introduction to the Theory of Computation","description":"Instructor: Harry Sha","sidebar":"tutorialSidebar"},"csc/csc236/Finite Automata":{"id":"csc/csc236/Finite Automata","title":"Finite Automata","description":"Basic definition:","sidebar":"tutorialSidebar"},"csc/csc236/Graph":{"id":"csc/csc236/Graph","title":"Graph","description":"A graph $G= (V, E)$ is a pair of sets $(V, E)$ where  $V$ is a set of vertices and $E$ is a set of edges.","sidebar":"tutorialSidebar"},"csc/csc236/Induction":{"id":"csc/csc236/Induction","title":"Induction","description":"Almost every courses teach induction","sidebar":"tutorialSidebar"},"csc/csc236/Recurrences":{"id":"csc/csc236/Recurrences","title":"Recurrences","description":"Review of time limit:","sidebar":"tutorialSidebar"},"csc/csc263/Amortized Analysis":{"id":"csc/csc263/Amortized Analysis","title":"Amortized Analysis","description":"We average the time that required to perform a sequence of data-strcuture operations over all the operations performed in the Amortized Analysis. That is, amortized analysis is also a time complexity analysis technique that is used to analyze the average time complexity of an algorithm or operation but without probabilistic analysis. In addtion, amortized analysis guarantees that the average performance of each operation in the worst case.","sidebar":"tutorialSidebar"},"csc/csc263/AVL Tree":{"id":"csc/csc263/AVL Tree","title":"AVL Tree","description":"A tree is an AVL Tree if it is binary search tree and AVL balanced.","sidebar":"tutorialSidebar"},"csc/csc263/Binary Counter":{"id":"csc/csc263/Binary Counter","title":"Binary Counter","description":"Binary Counter is just a counter use to count binary bits of a number, it is not a data structure or algorithm or something. Let use a array (if given the max-size of the number) or a list to store the bits of the number, since the idea that a integer $x$ can be present as $x = \\\\sum_{i = 0}^ A[i] * 2^i$, where $A[i]$ is the $i$-th bit of the number. It is useful to say how computer calculate the number.","sidebar":"tutorialSidebar"},"csc/csc263/Binary Search Tree":{"id":"csc/csc263/Binary Search Tree","title":"Binary Search Tree","description":"Let $x$ be a node in a Binary Search Tree(BST). If $y$ is a node in the left subtree of $x$, then $y.val   x.val$. That means we have a sorted list if we in-order travels (here the  stand for the priority).","sidebar":"tutorialSidebar"},"csc/csc263/Binomial Heap":{"id":"csc/csc263/Binomial Heap","title":"Binomial Heap","description":"Binomial Heap is a mergeable heap is a heap data structure implements by a collection of binomial trees follwing the binomial heap properties:","sidebar":"tutorialSidebar"},"csc/csc263/csc263":{"id":"csc/csc263/csc263","title":"CSC263 Data Structures and Analysis","description":"Instructor: Danny Heap","sidebar":"tutorialSidebar"},"csc/csc263/Dictionary":{"id":"csc/csc263/Dictionary","title":"Dictionary","description":"Dictionary is famous ADT in python (.net). In other programming language we call it map or something else. They are almost the same things, but called different in the different language. For each item in the collection dictionary, the item.key should be unique, that is, why we call it dictionary where a stuff usually have a unique name. For dictionary, we should have:","sidebar":"tutorialSidebar"},"csc/csc263/Disjoint Set":{"id":"csc/csc263/Disjoint Set","title":"Disjoint Set","description":"Sometime you may call Disjoint Set as Union-Find Set.","sidebar":"tutorialSidebar"},"csc/csc263/Dynamic Array":{"id":"csc/csc263/Dynamic Array","title":"Dynamic Array","description":"Actually we use dynamic array frequently, like list in python, vector in c++, ArrayList/List in Java","sidebar":"tutorialSidebar"},"csc/csc263/Graph":{"id":"csc/csc263/Graph","title":"Graph","description":"The adjacvency-list representation of a graph $G = (V,E)$ consists of an array  Adj  , one for each vertex in $V$. For each $u\\\\in V$, the adjacency list $Adj[u]$ contains all the vertices $\\\\nu$ such that $(u,\\\\nu)\\\\in E$.","sidebar":"tutorialSidebar"},"csc/csc263/Hash":{"id":"csc/csc263/Hash","title":"Hash","description":"Hash rather than a data structure, it\'s more like a algorithm. It provide $O(1)$ time complexity to quickly access the elements. We want to find a efficient bi-jection function to store data.","sidebar":"tutorialSidebar"},"csc/csc263/Heap":{"id":"csc/csc263/Heap","title":"Heap","description":"Heap is a binary tree with the max/min among its children value in its root,  and also, for all of its child tree, their root also is the max/min among their child tree.","sidebar":"tutorialSidebar"},"csc/csc263/Minimum Spanning Tree":{"id":"csc/csc263/Minimum Spanning Tree","title":"Minimum Spanning Tree","description":"We define a saft edge for a subset $A$ of some minimum spanning tree if adding it to $A$ still is a part of a minimum spanning tree.","sidebar":"tutorialSidebar"},"csc/csc263/Ordered Set":{"id":"csc/csc263/Ordered Set","title":"Ordered Set","description":"An augmented AVL Trees, where for each node we have a new attribute node named count.","sidebar":"tutorialSidebar"},"csc/csc263/Priority Queue":{"id":"csc/csc263/Priority Queue","title":"Priority Queue","description":"Priority Queue is an ADT that the item in it will be sorted in given priority. Basically, there are 3 operation of a priority queue where we can insert, find the max priority item and extract the max priority queue.","sidebar":"tutorialSidebar"},"csc/csc263/Quick Sort":{"id":"csc/csc263/Quick Sort","title":"Quick Sort","description":"When we talk about the quick sort method we always saids about the random quick sort. Quicksort basically is a divide and conquer algorithm. It works by selecting a \'pivot\' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. Those sub-arrays are then sorted recursively. The deterministic version will take a fix pivot, and the random version will take a random pivot.","sidebar":"tutorialSidebar"},"csc/csc369/00 intro":{"id":"csc/csc369/00 intro","title":"OS - three piece","description":"- virtualization - virtual machine","sidebar":"tutorialSidebar"},"csc/csc369/01 process":{"id":"csc/csc369/01 process","title":"Process","description":"process: the running program","sidebar":"tutorialSidebar"},"csc/csc369/02 system call":{"id":"csc/csc369/02 system call","title":"System Call","description":"As we talked in 0 OS isolate user programs from each other, but sometimes we need them communicate with each other. OS provides service to user programs through system call. System call is a function call that invokes the OS. Whenever an application wants to use a resource that the OS manages, it asks permission from the OS through a system call.","sidebar":"tutorialSidebar"},"csc/csc369/03 thread":{"id":"csc/csc369/03 thread","title":"Thread","description":"Processes have private virtual address space and are isolated from each other. But can be communicated to each other by signals, pipes, sockets, files, etc. Processes also can share their memory by system call like shmget(), shmat(), mmap(), etc.","sidebar":"tutorialSidebar"},"csc/csc369/04 Synchronization":{"id":"csc/csc369/04 Synchronization","title":"Synchronization","description":"Too avoid arbitrary interleaving of thread executions (where may leads unexpected consequences) we need synchronization where is the mechanism gives us this control. Some flavours of synchronization are:","sidebar":"tutorialSidebar"},"csc/csc369/05 Scheduling":{"id":"csc/csc369/05 Scheduling","title":"Scheduling","description":"We talked Scheduling before, but never introduce it formally. Scheduling is the allocation of processors to processes (or threads) over time. It\'s the key of multiprogramming. Why?","sidebar":"tutorialSidebar"},"csc/csc369/06 Memory Management":{"id":"csc/csc369/06 Memory Management","title":"Memory Management","description":"Process address space layout","sidebar":"tutorialSidebar"},"csc/csc369/07 Paging and Address Translation":{"id":"csc/csc369/07 Paging and Address Translation","title":"Paging and Address Translation","description":"We need relocation, but how we do it?","sidebar":"tutorialSidebar"},"csc/csc369/08 Principles and Policies for Memory Management":{"id":"csc/csc369/08 Principles and Policies for Memory Management","title":"Principles and Policies for Memory Management","description":"Locality","sidebar":"tutorialSidebar"},"csc/csc369/09 file system introduction":{"id":"csc/csc369/09 file system introduction","title":"File System","description":"File system is used to virtualize persistent storage.","sidebar":"tutorialSidebar"},"csc/csc369/10 file system reliability":{"id":"csc/csc369/10 file system reliability","title":"File System Reliability","description":"We need to consider the onsistency issues about fs.","sidebar":"tutorialSidebar"},"csc/csc369/csc369":{"id":"csc/csc369/csc369","title":"CSC369: Operating Systems","description":"Course Overview:","sidebar":"tutorialSidebar"},"csc/csc413/Backpropagation":{"id":"csc/csc413/Backpropagation","title":"Backpropagation","description":"We define backpropagation as the reverse mode automatic differentiation. Or basically, it is the chain rule applied to neural networks.","sidebar":"tutorialSidebar"},"csc/csc413/CNN":{"id":"csc/csc413/CNN","title":"CNN and Image Classification","description":"When we do learn image, the larger image may lead the more parameters and more computation. But we don\'t actually need to looks at the entire image and we want the incoming weights to focus on local patterns of input image. We would like to use high-level operation convolution to extract features from the image.","sidebar":"tutorialSidebar"},"csc/csc413/csc413":{"id":"csc/csc413/csc413","title":"CSC413 Neural Networks and Deep Learning","description":"Instructor: Jimmy Ba, Bo Wang","sidebar":"tutorialSidebar"},"csc/csc413/Intro to NN":{"id":"csc/csc413/Intro to NN","title":"Intro to NN","description":"Introduction to Neural Networks","sidebar":"tutorialSidebar"},"csc/csc413/Language Model":{"id":"csc/csc413/Language Model","title":"Language Model","description":"Learning a good distribution $p(s)$ of sentences. Such problem is called language modeling.","sidebar":"tutorialSidebar"},"csc/csc413/Multilayer Perceptrons":{"id":"csc/csc413/Multilayer Perceptrons","title":"Multi-Layer Perceptrons","description":"We define layers by some grouped units. Each unit here in graph is a neuron node. The input layer is the first layer, and the output layer is the last layer. The hidden layers are the layers between input and output layers.","sidebar":"tutorialSidebar"},"csc/csc413/Optimization":{"id":"csc/csc413/Optimization","title":"Optimization","description":"How do we train models using gradient? And how do we solve the issues of gradient (i.e. learning rate). One way is to do derivative again. That is, We have Hessian Matrix which is a symmetric matrix.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Intro","description":"This page lists all the courses I have taken or am currently taking.  The notes may not complete and may not meant to be a substitute for the course material. They are just my personal notes that I have taken while taking the course. I have also included the links to the course material if it is available online.","sidebar":"tutorialSidebar"},"mat/mat301/0 Equivalence Relations":{"id":"mat/mat301/0 Equivalence Relations","title":"Equivalence Relations","description":"An equivalence relation on set on a set X is a relation ~ on the elements of X which satisfies the following properties:","sidebar":"tutorialSidebar"},"mat/mat301/1 Group Definition":{"id":"mat/mat301/1 Group Definition","title":"Group Definition","description":"Binary operation: a function map $G\\\\times G \\\\to G$, denote such function by $\\\\cdot$ then we can write $\\\\cdot(x,y)$ or $x\\\\cdot y$ to present the operation acting on $x,y$.","sidebar":"tutorialSidebar"},"mat/mat301/2 Cyclic Group":{"id":"mat/mat301/2 Cyclic Group","title":"Cyclic Group","description":"We define $G = \\\\{a^n:n\\\\in\\\\Z\\\\} = \\\\langle a \\\\rangle$ is a Cyclic Group.","sidebar":"tutorialSidebar"},"mat/mat301/3 Permutation Group":{"id":"mat/mat301/3 Permutation Group","title":"Permutation Group","description":"Permutation of a set is a bijection function $A\\\\to A$.","sidebar":"tutorialSidebar"},"mat/mat301/4 Isomorphisms":{"id":"mat/mat301/4 Isomorphisms","title":"Group Isomorphisms","description":"Isomorphism G\\\\to G\', \\\\varphi(ab) = \\\\varphi(a)\\\\varphi(b) \\\\implies \\\\varphi$ is bijective, then $\\\\varphi$ is a isomorphism.","sidebar":"tutorialSidebar"},"mat/mat301/5 Cosets":{"id":"mat/mat301/5 Cosets","title":"Cosets","description":"Let $G$ be a group, and let $H\\\\subset G$ be a subset(subgroup better). $\\\\forall a\\\\in G$:","sidebar":"tutorialSidebar"},"mat/mat301/6 Direct Products":{"id":"mat/mat301/6 Direct Products","title":"Direct Products","description":"Let $(G, ), (H, \\\\circ)$ be two groups. Let $(g_1,h_1)\\\\star (g_2,h_2) = (g_1g2, h1\\\\circ h_2)$, then $(G\\\\times H, \\\\star)$ is a group. $G\\\\times H$ called direct product of $G$ and $H$.","sidebar":"tutorialSidebar"},"mat/mat301/6 Normal subgroups":{"id":"mat/mat301/6 Normal subgroups","title":"Normal subgroups","description":"Let $H\\\\subset G$ be a subgroup,","sidebar":"tutorialSidebar"},"mat/mat301/7 Homomorphisms":{"id":"mat/mat301/7 Homomorphisms","title":"Group Homomorphisms","description":"Let $G, \\\\overline$ be groups, $\\\\phi: G\\\\to \\\\overline{G}$ be a mapping, $\\\\forall x,y\\\\in G, \\\\phi(xy) = \\\\phi(x)\\\\phi(y) \\\\implies \\\\phi$ is a homomorphism.","sidebar":"tutorialSidebar"},"mat/mat301/8 Fundamental Theorem of Finite Abelian Groups":{"id":"mat/mat301/8 Fundamental Theorem of Finite Abelian Groups","title":"Fundamental Theorem of Finite Abelian Groups","description":"Fundamental Theorem of Finite Abelian Groups states that: every finite Abelian group is a direct product of cyclic groups of prime power order. Moreover, the number of terms in the product and the orders of the cyclic groups are uniquely determined by the group.","sidebar":"tutorialSidebar"},"mat/mat301/8 Sylow Theorem":{"id":"mat/mat301/8 Sylow Theorem","title":"Sylow Theorems","description":"Def: Let $a,b\\\\in G$, we say $a$ and $b$ are conjugate in $G$ if $xax^=b$ for some $x\\\\in G$.","sidebar":"tutorialSidebar"},"mat/mat301/Group Action":{"id":"mat/mat301/Group Action","title":"Group Action","description":"An action of a group $G$ on a set $X$ is a map $\\\\varphi","sidebar":"tutorialSidebar"},"mat/mat301/Group Class":{"id":"mat/mat301/Group Class","title":"More Groups","description":"$Dn$ : Let $n \\\\ge 3$, **Dihedral group** is a symmetrical regular n-gon denote by $Dn$","sidebar":"tutorialSidebar"},"mat/mat301/mat301":{"id":"mat/mat301/mat301","title":"MAT301 Groups and Symmetries","description":"Instructor: Sarah Mayes-Tang","sidebar":"tutorialSidebar"},"mat/mat337/Differential equations and integral calculus":{"id":"mat/mat337/Differential equations and integral calculus","title":"Differential equations and integral calculus","description":"A real-valued function $f$ is said to be differentiable at a point $x0$ if $\\\\lim\\\\limits{h\\\\to 0} \\\\frac{f(x0+h)-f(x0)} = \\\\lim\\\\limits{x\\\\to x0} \\\\frac{f(x)-f(x0)}{x-x0}$ exists. The function is differentiable if it is differentiable at every point. The limit is called the derivative of $f$ at $x0$ and is denoted by $f\'(x0)$ or $\\\\frac{d}{dx}f(x_0)$.","sidebar":"tutorialSidebar"},"mat/mat337/Function":{"id":"mat/mat337/Function","title":"Functions","description":"In other course, the function we use is usually $f \\\\R^n \\\\to \\\\R^m$ so that all the properties may differ and advance than in $\\\\R$","sidebar":"tutorialSidebar"},"mat/mat337/Introduction to Real Number":{"id":"mat/mat337/Introduction to Real Number","title":"Introduction to Real Number","description":"Beyond real number, let\'s review some about rational number first. Rational number a.k.a the extension of natural number where we want to separate a integer.","sidebar":"tutorialSidebar"},"mat/mat337/Limits of Functions":{"id":"mat/mat337/Limits of Functions","title":"Limits of Functions","description":"We learn lots about sequence, series, functions and their convergence, cauchy, limits and etc. But what if you combine them all?","sidebar":"tutorialSidebar"},"mat/mat337/mat337":{"id":"mat/mat337/mat337","title":"MAT337 Introduction to Real Analysis","description":"Instructor: Ignacio Uriarte-Tuero","sidebar":"tutorialSidebar"},"mat/mat337/Metric Spaces":{"id":"mat/mat337/Metric Spaces","title":"Metric Spaces","description":"Similar like the norm, we define the metric on a set $X$ is a function $\\\\rho$ on $X\\\\times X$ in $[0,\\\\infty)$ satisfying the following conditions:","sidebar":"tutorialSidebar"},"mat/mat337/Norms":{"id":"mat/mat337/Norms","title":"Norms","description":"In the previous section, we study the Euclidean norm, but how about the general norm?","sidebar":"tutorialSidebar"},"mat/mat337/Sequence":{"id":"mat/mat337/Sequence","title":"Sequence","description":"$\\\\forall n\\\\in \\\\mathbb$ we define the list number $\\\\dots,xn,x{n+1},\\\\dots$ is a sequence. We use $(xn)$ to denote; sometimes we have multiple variable and we may have something like $(x{a,b})_{a\\\\in N}$ means this sequence focus on the changing of $a$","sidebar":"tutorialSidebar"},"mat/mat337/Series":{"id":"mat/mat337/Series","title":"Series","description":"Let $(an)^{\\\\infty}{n=1}$ be a sequence of numbers, we define a infinite series with terms $an$ is the formal expression $\\\\sum{n= 1}^{\\\\infty} an$. Define a sequence of **partial sums** $(sn){n=1}^{\\\\infty}$ by $sn = \\\\sum{k=1}^n ak$","sidebar":"tutorialSidebar"},"mat/mat337/Topology Of N Demensional Real Number":{"id":"mat/mat337/Topology Of N Demensional Real Number","title":"Topology Of N Demensional Real Number","description":"We define a $1\\\\times n$ vector in $\\\\R^n$ is a point where a zero vector is all $0$","sidebar":"tutorialSidebar"},"sta/sta314/Basic ML info":{"id":"sta/sta314/Basic ML info","title":"Parametric V.S. Non-parametric","description":"The reason why we do machine learning is that we want to have a model that predict/classify/..., but the fact is that every actual fact may exists errors, that is, we hard to have a perfect model.","sidebar":"tutorialSidebar"},"sta/sta314/Beyond Linearity":{"id":"sta/sta314/Beyond Linearity","title":"Moving beyond Linearity","description":"We always make linear assumption to assume a model which make our life easizer. However, linear assumption is not always a good approximation, and sometimes even a poor one. That is, we extend the linear model by feature.","sidebar":"tutorialSidebar"},"sta/sta314/Classfication":{"id":"sta/sta314/Classfication","title":"Classification","description":"We may have a classification problem for a qualitative result from an unordered set $C$. The main goal of us is to:","sidebar":"tutorialSidebar"},"sta/sta314/Decision Tree":{"id":"sta/sta314/Decision Tree","title":"Decision Tree","description":"Decision Tree is a supervised learning algorithm that can be used for both classification and regression problems. It is a tree-like structure where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node holds a class label. The paths from root to leaf represent classification rules. And generally, a decision tree has a high variance and low bias.","sidebar":"tutorialSidebar"},"sta/sta314/Discriminant Analysis":{"id":"sta/sta314/Discriminant Analysis","title":"Discriminant Analysis","description":"Discriminant Anaylsis parametrizes the distribution of $X | Y = 1$ and $X | Y = 0$.","sidebar":"tutorialSidebar"},"sta/sta314/Fitted Model Measurement":{"id":"sta/sta314/Fitted Model Measurement","title":"Fitted Model Measurement","description":"To measure the fit of a model, we need to compare the model\'s prediction with the actual data. The most common way to do this is to use the mean squared error (MSE) or the root mean squared error (RMSE). Depend on the different type of data, we can use different measurement to fit.","sidebar":"tutorialSidebar"},"sta/sta314/Gradient Descent":{"id":"sta/sta314/Gradient Descent","title":"Gradient Descent","description":"Gradient descent is a optimization iterative algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent is to do derivatives to find the critical point. It asks a convex function to execut. That is, we can use grandient descent to find the minimum of a MSE.","sidebar":"tutorialSidebar"},"sta/sta314/Lasso Regression":{"id":"sta/sta314/Lasso Regression","title":"Lasso Regression","description":"The key point of Lasso Regression is to shrinks the coefficients toward 0 by penalizing their absolute values whereas find a model minimize $[\\\\sum{i=1}^n (yi - \\\\beta0 - \\\\sum{j = 1}^p\\\\betaj x)^2] + \\\\lambda\\\\sum{j = 1}^p|\\\\betaj|= RSS + \\\\lambda\\\\sum{j = 1}^p|\\\\betaj|$","sidebar":"tutorialSidebar"},"sta/sta314/Linear Model":{"id":"sta/sta314/Linear Model","title":"Linear Model","description":"Let $Y = f(X) + \\\\epsilon$ be a linear model, then it must have the form: $Y = \\\\beta0+\\\\beta1X1 +\\\\ldots + \\\\betapX_p + \\\\epsilon$","sidebar":"tutorialSidebar"},"sta/sta314/Logistic Regression":{"id":"sta/sta314/Logistic Regression","title":"Logistic Regression","description":"Logistic regression is a parametric approach to classification. It gives a structure of probability of $x$ by $p(X) =\\\\frac{e^{\\\\beta0 + \\\\beta X}}{1+e^{\\\\beta0 + \\\\beta X}}$ where $\\\\beta0$ is the intercept and $\\\\beta$ is the coefficient matrix. Where $\\\\frac{p(X)}{1-p(X)} = e^{\\\\beta0 + \\\\beta X}$ is the odds.","sidebar":"tutorialSidebar"},"sta/sta314/Ordinary Least Square and Model Selection":{"id":"sta/sta314/Ordinary Least Square and Model Selection","title":"Ordinary Least Square and Model Selection","description":"Ordinary Least Squares(OLS) is a approach to find $\\\\hat$ on linear model where $\\\\hat f(X) = \\\\hat\\\\beta X$. That is, the things we actually to do is to find $\\\\hat\\\\beta$ where $\\\\hat \\\\beta = \\\\arg\\\\min\\\\limits{\\\\alpha\\\\in \\\\R^{p+1}} \\\\frac{1}{n}\\\\sum{i=1}^n (yi - xi^T \\\\alpha)^2 = \\\\arg\\\\min\\\\limits{\\\\alpha\\\\in \\\\R^{p+1}} \\\\frac{1}{n}\\\\sum{i=1}^n ||yi - X \\\\alpha||^22$","sidebar":"tutorialSidebar"},"sta/sta314/Ridge Regression":{"id":"sta/sta314/Ridge Regression","title":"Ridge Regression","description":"The key point of Ridge Regression is to find a model minimize $[\\\\sum{i=1}^n (yi - \\\\beta0 - \\\\sum{j = 1}^p\\\\betaj x)^2] + \\\\lambda\\\\sum{j = 1}^p\\\\betaj^2= RSS + \\\\lambda\\\\sum{j = 1}^p\\\\betaj^2$ where shrinks the coefficients toward 0","sidebar":"tutorialSidebar"},"sta/sta314/Situation without the Test Data to do Model Validation":{"id":"sta/sta314/Situation without the Test Data to do Model Validation","title":"Situation without the Test Data to do Model Validation","description":"There are two common approaches for model selection when we don\'t have $D_$","sidebar":"tutorialSidebar"},"sta/sta314/sta314":{"id":"sta/sta314/sta314","title":"STA314 Introduction to Machine/statisitcal Learning","description":"Instructor: Xin Bing \u90b4\u6b23","sidebar":"tutorialSidebar"},"sta/sta314/Support Vector Machine":{"id":"sta/sta314/Support Vector Machine","title":"Support Vector Machine","description":"The trainning pointes with equality constraints $yi(xi^Tw + b) \\\\ge M$ are called support vectors. We have Support Vector Machine (SVM) which is a classifier that finds the optimal hyperplane that separates the classes. SVM-like algorithms are often called max-margin or large-margin. Since the Primal-formulation is convex specially is a quadratic program. We can use SGD/GD to solve it. And its more common to solved by dual formulation.","sidebar":"tutorialSidebar"},"sta/sta314/Tree Improving":{"id":"sta/sta314/Tree Improving","title":"Tree Improving","description":"We have many ways to improve the tree, like pruning, boosting, bagging, etc. In this section, we will introduce the bagging, random forest, and boosting.","sidebar":"tutorialSidebar"},"sta/sta314/Unsupervised Model":{"id":"sta/sta314/Unsupervised Model","title":"Some Unsupervised Learning Model","description":"Unsupervised learning is a the study without labels, normally, is the task of grouping, explaining and finding structured data.","sidebar":"tutorialSidebar"},"sta/sta347/0 Introduction to Probabilities":{"id":"sta/sta347/0 Introduction to Probabilities","title":"Basic Properties of Probabilities","description":"We use $\\\\Omega$ or $S$ present the probability space, $\\\\mathbb$ present some other space;","sidebar":"tutorialSidebar"},"sta/sta347/1 Random Variable":{"id":"sta/sta347/1 Random Variable","title":"Random Variable","description":"A random variable is function from the  sample space $S$ to the set of all real numbers $\\\\R$. we can denote such function as $X(s): S\\\\to \\\\R$","sidebar":"tutorialSidebar"},"sta/sta347/2 Expected Values, Variance and Momentum":{"id":"sta/sta347/2 Expected Values, Variance and Momentum","title":"Expected Values","description":"Expectation is a location measure which give the location of the center of a random variable.","sidebar":"tutorialSidebar"},"sta/sta347/3 Random Variables Convergence":{"id":"sta/sta347/3 Random Variables Convergence","title":"Random Variables Convergence","description":"From the analysis courses, we somehow learn the convergence of sequence, the convergence of series, etc. Probability as the branch of matrics is also a branch of analysis. So, we can also talk about the convergence of probability.","sidebar":"tutorialSidebar"},"sta/sta347/4 Change of Variable":{"id":"sta/sta347/4 Change of Variable","title":"Change of Variable","description":"Discrete","sidebar":"tutorialSidebar"},"sta/sta347/4 Distribution":{"id":"sta/sta347/4 Distribution","title":"Distributions","description":"Discrete Distributions","sidebar":"tutorialSidebar"},"sta/sta347/4 Inequality":{"id":"sta/sta347/4 Inequality","title":"Inequality","description":"(BOOLE\'S INEQUALITY): $P(\\\\bigcup{k=1}^{\\\\infty} Ak)\\\\le \\\\sum{k=1} ^{\\\\infty} P(Ak)$","sidebar":"tutorialSidebar"},"sta/sta347/5 Stochastic Process":{"id":"sta/sta347/5 Stochastic Process","title":"Stochastic Process","description":"If random variables represent a process that proceed randomly in time, then it\'s Stochastic Process.","sidebar":"tutorialSidebar"},"sta/sta347/6 Markov Chain":{"id":"sta/sta347/6 Markov Chain","title":"Markov Chain","description":"Markov Chain theorems are concerned with what will happen in a long run.","sidebar":"tutorialSidebar"},"sta/sta347/7 Monte Carlo Approximation":{"id":"sta/sta347/7 Monte Carlo Approximation","title":"Monte Carlo Approximation","description":"Let $X1, X2, \\\\ldots$ be a sequence of i.i.d. random variables with mean $\\\\mu$ we have $Mn = \\\\frac{n} \\\\sum{i=1}^n Xi$ and LLN tells us that $Mn \\\\approx \\\\mu$ as $n \\\\to \\\\infty$. If we dont know the $\\\\mu$, we can use $M_n$ as an estimator or approximation of $\\\\mu$. This is called Monte Carlo approximation.","sidebar":"tutorialSidebar"},"sta/sta347/Recap":{"id":"sta/sta347/Recap","title":"Review","description":"Real analysis","sidebar":"tutorialSidebar"},"sta/sta347/sta347":{"id":"sta/sta347/sta347","title":"STA347 Probability","description":"Instructor: Mohammad Kaviul Khan","sidebar":"tutorialSidebar"},"sta/sta414/0 Bayes Nets":{"id":"sta/sta414/0 Bayes Nets","title":"Conditional Independence and Bayes Nets","description":"denote set $xA = \\\\{xi","sidebar":"tutorialSidebar"},"sta/sta414/1 Markov Random Fields":{"id":"sta/sta414/1 Markov Random Fields","title":"Markov Random Fields","description":"Markov Blanket (MB): the set of nodes that makes $X_i$ conditionally independent of the other nodes.","sidebar":"tutorialSidebar"},"sta/sta414/2 Probabilistic Graphical Models":{"id":"sta/sta414/2 Probabilistic Graphical Models","title":"Probabilistic Graphical Models","description":"We introduce the concept of probabilistic graphical models (PGMs) as a probabilistic model for representing the dconditional dependence structure between random variables. Some of the most common PGMs are Markov Random Fields and Bayesian Networks","sidebar":"tutorialSidebar"},"sta/sta414/3 Sampling":{"id":"sta/sta414/3 Sampling","title":"Sampling","description":"We have multiple ways to do sampling.","sidebar":"tutorialSidebar"},"sta/sta414/4 Hidden Markov Model":{"id":"sta/sta414/4 Hidden Markov Model","title":"Hidden Markov Model","description":"In previous courses or even previous lecture, we always generally assume data was i.i.d. for convenience purpose, however this may be a poor assumption. Many real life problems are not i.i.d. instead they are sequential data. That is, we make the simplifying assumption that our data can be modeled as a first-order Markov chain $p(xt|x{1:t-1}) = p(xt|x)$.","sidebar":"tutorialSidebar"},"sta/sta414/5 Variational Inference":{"id":"sta/sta414/5 Variational Inference","title":"Variational Inference","description":"Recall the posterior distribution $p(z|x) = \\\\frac{p(x,z)}{p(x)}$ is the distribution of the latent variables given the observed data where $p(x) = \\\\int p(x,z) dz$ is the marginal distribution of the observed data. But generally, when we face high dimensional latent variables, it becomes intractable to compute the posterior distribution. Specifically, we have the following problem:","sidebar":"tutorialSidebar"},"sta/sta414/6 Gaussians Mixture Model":{"id":"sta/sta414/6 Gaussians Mixture Model","title":"Mixture of Gaussians (or Gaussian Mixture Model (GMM))","description":"We use GMM when the situation that Gaussian latent variable model $p(x) = \\\\sum_z p(x, z)$ used for clustering.","sidebar":"tutorialSidebar"},"sta/sta414/7 Probabilistic PCA":{"id":"sta/sta414/7 Probabilistic PCA","title":"Probabilistic Principal Component Analysis","description":"Sometimes data is very high dimensional, its important features can be accurately captured in a low dimensional subspace. That is the purpose we use PCA.","sidebar":"tutorialSidebar"},"sta/sta414/8 Bayesian Linear Regression":{"id":"sta/sta414/8 Bayesian Linear Regression","title":"Bayesian Linear Regression","description":"BLR is used when Gaussian discriminative model $p(y|X)$ used for regression with a Bayesian analysis for the weights.","sidebar":"tutorialSidebar"},"sta/sta414/9 Kernal Method":{"id":"sta/sta414/9 Kernal Method","title":"Kernal Method","description":"Define $\\\\psi(x): \\\\R^D \\\\to \\\\R^M$ and input data $X \\\\in \\\\R^{N \\\\times D}$ and $\\\\Psi \\\\in \\\\R^{N \\\\times M}$, where $\\\\Psi = \\\\psi(X)$. Then we have the prediction matrix $\\\\hat = \\\\Psi w$. Then $y|x \\\\sim N(w^T\\\\psi(x), \\\\sigma^2)$.","sidebar":"tutorialSidebar"},"sta/sta414/Recap":{"id":"sta/sta414/Recap","title":"Some recap of previous courses","description":"Sufficient Statistics","sidebar":"tutorialSidebar"},"sta/sta414/sta414":{"id":"sta/sta414/sta414","title":"STA414 Statistical Methods for Machine Learning II","description":"Instructor: Piotr Zwiernik, Murat A. Erdogdu","sidebar":"tutorialSidebar"},"sta/sta437/Basic Information to Multivariate Data":{"id":"sta/sta437/Basic Information to Multivariate Data","title":"Basic Information to Multivariate Data","description":"For a multivariate data, denote it with $p$ variables where $p \\\\ge 2$, and with $n$ observations(item/experimental unit). We also denote as $x_$ where the measurement of kth variable on the jth item or experimental unit.","sidebar":"tutorialSidebar"},"sta/sta437/Multivariate Data Analysis among Machine Learning":{"id":"sta/sta437/Multivariate Data Analysis among Machine Learning","title":"Multivariate Data Analysis among Machine Learning","description":"Linear Regression","sidebar":"tutorialSidebar"},"sta/sta437/Multivariate Normal Distribution":{"id":"sta/sta437/Multivariate Normal Distribution","title":"Multivariate Normal Distribution","description":"Multivariate Normal Distribution is a generalization of the normal distribution to multiple dimensions. It is often a good approximation to the true distribution where by Central Limit Theorem, multivariate normal distribution is the sample distribution of many multivariate random variables.","sidebar":"tutorialSidebar"},"sta/sta437/sta437":{"id":"sta/sta437/sta437","title":"STA437 Method for Multivariate Data Analysis","description":"Instructor: Mehdi Molkaraie","sidebar":"tutorialSidebar"},"sta/sta447/0 Markov Chain":{"id":"sta/sta447/0 Markov Chain","title":"Markov Chain","description":"Most of the markov chain property and defintion is the same as Markov Chain. And those difference list in this page.","sidebar":"tutorialSidebar"},"sta/sta447/0.5 Simple Random Walk":{"id":"sta/sta447/0.5 Simple Random Walk","title":"Simple Random Walk","description":"Let the state space be $\\\\mathcal = \\\\Z$, a process $Xt$ has probability $p$ moving forward and probability $1-p$ moving backward (i.e. $p{i, i+1} = p$ and $p_{i, i-1} = 1-p$). The process is called a simple random walk.","sidebar":"tutorialSidebar"},"sta/sta447/1 Markov Chain Behavior":{"id":"sta/sta447/1 Markov Chain Behavior","title":"Markov Chain Behavior","description":"As we learn more about Markov chains (especially the infinite ones), we would like to find out the behavior of the chain. One of the interesting behavior is the stationary distribution.","sidebar":"tutorialSidebar"},"sta/sta447/2 Markov Chain Monte Carlo":{"id":"sta/sta447/2 Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","description":"Markov chain Monte Carlo (MCMC) is a methodology that use Markov sequences to efficiently model otherwise intractable distributions. That is, Given a probability distribution $\\\\pi$, the goal of MCMC is to simulate a random variable $X$ whose distribution is $\\\\pi$. Such distribution may continuous or discrete even though we assume it\'s discrete.","sidebar":"tutorialSidebar"},"sta/sta447/3 Martingales":{"id":"sta/sta447/3 Martingales","title":"Martingales","description":"Recall the Martingales we learn in STA 347. In this section, we will review the definition of a martingale and some of its properties.","sidebar":"tutorialSidebar"},"sta/sta447/4 Continuous Processes":{"id":"sta/sta447/4 Continuous Processes","title":"Continuous Processes","description":"In previous section, we discussed the discrete-time processes. We now define a continuous time stochastic process $Xt$ has **Markov property** if $P(Xt = y| Xr, 0 \\\\le r \\\\le s) = P(Xt = y| Xs)$ for all $t \\\\ge s$ and **Time-homogeneous** if $P(X{t+s} = y| Xs = x) = P(Xt = y| X_0 = x)$.","sidebar":"tutorialSidebar"},"sta/sta447/sta447":{"id":"sta/sta447/sta447","title":"STA447 Stochastic Processe","description":"Instructor: Omidali (Omid) Aghababaei Jazi","sidebar":"tutorialSidebar"}}}')}}]);