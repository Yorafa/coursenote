<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">Machine Learning | Yorafa&#x27;s Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://note.yorafa.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://note.yorafa.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://note.yorafa.com/docs/category/machine-learning"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Machine Learning | Yorafa&#x27;s Note"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://note.yorafa.com/docs/category/machine-learning"><link data-rh="true" rel="alternate" href="https://note.yorafa.com/docs/category/machine-learning" hreflang="en"><link data-rh="true" rel="alternate" href="https://note.yorafa.com/docs/category/machine-learning" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9bb80656.css">
<script src="/assets/js/runtime~main.a3aeb8e8.js" defer="defer"></script>
<script src="/assets/js/main.23ce9542.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Yorafa&#x27;s Note</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Notes</a><a href="https://home.yorafa.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Nav</a><a href="https://yorafa.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/Yorafa/coursenote" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/basic">Basic</a><button aria-label="Expand sidebar category &#x27;Basic&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/algorithm">Algorithm</a><button aria-label="Expand sidebar category &#x27;Algorithm&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/deep-learning">Deep Learning</a><button aria-label="Expand sidebar category &#x27;Deep Learning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="menu__link menu__link--sublist menu__link--active" aria-current="page" href="/docs/category/machine-learning">Machine Learning</a><button aria-label="Collapse sidebar category &#x27;Machine Learning&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/0 Bayes Nets">Conditional Independence and Bayes Nets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/1 Linear Regression">Linear Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/1 Markov Random Fields">Markov Random Fields</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/2 Probabilistic Graphical Models">Probabilistic Graphical Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/3 Sampling">Sampling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/4 Hidden Markov Model">Hidden Markov Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/5 Variational Inference">Variational Inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/6 Gaussians Mixture Model">Mixture of Gaussians (or Gaussian Mixture Model (GMM))</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/7 Probabilistic PCA">Probabilistic Principal Component Analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/8 Bayesian Linear Regression">Bayesian Linear Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/9 Kernal Method">Kernal Method</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Basic Information to Multivariate Data">Basic Information to Multivariate Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Beyond Linearity">Moving beyond Linearity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Classfication">Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Decision Tree">Decision Tree</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Discriminant Analysis">Discriminant Analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Fitted Model Measurement">Fitted Model Measurement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Gradient Descent">Gradient Descent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Lasso Regression">Lasso Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Logistic Regression">Logistic Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning">Machine Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Model Selection">Model Selection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Multivariate Data Analysis among Machine Learning">Multivariate Data Analysis among Machine Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Multivariate Normal Distribution">Multivariate Normal Distribution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Recap">Some recap of previous courses</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Ridge Regression">Ridge Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Situation without the Test Data to do Model Validation">Situation without the Test Data to do Model Validation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Support Vector Machine">Support Vector Machine</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Tree Improving">Tree Improving</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/Unsupervised Model">Some Unsupervised Learning Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/sta414">STA414 Statistical Methods for Machine Learning II</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Machine Learning/sta437">STA437 Method for Multivariate Data Analysis</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/nlp">NLP</a><button aria-label="Expand sidebar category &#x27;NLP&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/mat">MAT</a><button aria-label="Expand sidebar category &#x27;MAT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/category/sta">STA</a><button aria-label="Expand sidebar category &#x27;STA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/About">About</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Blog Backup/UTLeetcoder">Blog Backup</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Database">Database</a><button aria-label="Expand sidebar category &#x27;Database&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/DevOps/AWS/Outline">DevOps</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Leetcode/1063">Leetcode</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/Operating Systems">Operating Systems</a><button aria-label="Expand sidebar category &#x27;Operating Systems&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/å°æ•…äº‹">S(Situation)T(Task)A(Action)R(Result)</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="generatedIndexPage_vN6x"><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Machine Learning</span><meta itemprop="position" content="1"></li></ul></nav><header><h1 class="title_kItE">Machine Learning</h1></header><article class="margin-top--lg"><section class="row list_eTzJ"><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/0 Bayes Nets"><h2 class="text--truncate cardTitle_rnsV" title="Conditional Independence and Bayes Nets">ğŸ“„ï¸<!-- --> <!-- -->Conditional Independence and Bayes Nets</h2><p class="text--truncate cardDescription_PWke" title="denote set $xA = \{xi">denote set $xA = \{xi</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/1 Linear Regression"><h2 class="text--truncate cardTitle_rnsV" title="Linear Regression">ğŸ“„ï¸<!-- --> <!-- -->Linear Regression</h2><p class="text--truncate cardDescription_PWke" title="Recall $Y = f(X) + \epsilon$. Now we first assume it&#x27;s a linear model, then $Y = \beta0+\beta1X1 +\ldots + \betapXp + \epsilon$ where $f(X) = \beta0+\beta1X1 +\ldots + \betapXp$ is a linear combination of the predictors $X1, X2, \ldots, X_p$ which shows the linear relationship between $X$ and $Y$, that is, the linear regression model.">Recall $Y = f(X) + \epsilon$. Now we first assume it&#x27;s a linear model, then $Y = \beta0+\beta1X1 +\ldots + \betapXp + \epsilon$ where $f(X) = \beta0+\beta1X1 +\ldots + \betapXp$ is a linear combination of the predictors $X1, X2, \ldots, X_p$ which shows the linear relationship between $X$ and $Y$, that is, the linear regression model.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/1 Markov Random Fields"><h2 class="text--truncate cardTitle_rnsV" title="Markov Random Fields">ğŸ“„ï¸<!-- --> <!-- -->Markov Random Fields</h2><p class="text--truncate cardDescription_PWke" title="Markov Blanket (MB): the set of nodes that makes $X_i$ conditionally independent of the other nodes.">Markov Blanket (MB): the set of nodes that makes $X_i$ conditionally independent of the other nodes.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/2 Probabilistic Graphical Models"><h2 class="text--truncate cardTitle_rnsV" title="Probabilistic Graphical Models">ğŸ“„ï¸<!-- --> <!-- -->Probabilistic Graphical Models</h2><p class="text--truncate cardDescription_PWke" title="We introduce the concept of probabilistic graphical models (PGMs) as a probabilistic model for representing the conditional dependence structure between random variables. Some of the most common PGMs are Markov Random Fields and Bayesian Networks">We introduce the concept of probabilistic graphical models (PGMs) as a probabilistic model for representing the conditional dependence structure between random variables. Some of the most common PGMs are Markov Random Fields and Bayesian Networks</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/3 Sampling"><h2 class="text--truncate cardTitle_rnsV" title="Sampling">ğŸ“„ï¸<!-- --> <!-- -->Sampling</h2><p class="text--truncate cardDescription_PWke" title="We have multiple ways to do sampling.">We have multiple ways to do sampling.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/4 Hidden Markov Model"><h2 class="text--truncate cardTitle_rnsV" title="Hidden Markov Model">ğŸ“„ï¸<!-- --> <!-- -->Hidden Markov Model</h2><p class="text--truncate cardDescription_PWke" title="In previous courses or even previous lecture, we always generally assume data was i.i.d. for convenience purpose, however this may be a poor assumption. Many real life problems are not i.i.d. instead they are sequential data. That is, we make the simplifying assumption that our data can be modeled as a first-order Markov chain $p(xt|x{1:t-1}) = p(xt|x)$.">In previous courses or even previous lecture, we always generally assume data was i.i.d. for convenience purpose, however this may be a poor assumption. Many real life problems are not i.i.d. instead they are sequential data. That is, we make the simplifying assumption that our data can be modeled as a first-order Markov chain $p(xt|x{1:t-1}) = p(xt|x)$.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/5 Variational Inference"><h2 class="text--truncate cardTitle_rnsV" title="Variational Inference">ğŸ“„ï¸<!-- --> <!-- -->Variational Inference</h2><p class="text--truncate cardDescription_PWke" title="Recall the posterior distribution $p(z|x) = \frac{p(x,z)}{p(x)}$ is the distribution of the latent variables given the observed data where $p(x) = \int p(x,z) dz$ is the marginal distribution of the observed data. But generally, when we face high dimensional latent variables, it becomes intractable to compute the posterior distribution. Specifically, we have the following problem:">Recall the posterior distribution $p(z|x) = \frac{p(x,z)}{p(x)}$ is the distribution of the latent variables given the observed data where $p(x) = \int p(x,z) dz$ is the marginal distribution of the observed data. But generally, when we face high dimensional latent variables, it becomes intractable to compute the posterior distribution. Specifically, we have the following problem:</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/6 Gaussians Mixture Model"><h2 class="text--truncate cardTitle_rnsV" title="Mixture of Gaussians (or Gaussian Mixture Model (GMM))">ğŸ“„ï¸<!-- --> <!-- -->Mixture of Gaussians (or Gaussian Mixture Model (GMM))</h2><p class="text--truncate cardDescription_PWke" title="We use GMM when the situation that Gaussian latent variable model $p(x) = \sum_z p(x, z)$ used for clustering.">We use GMM when the situation that Gaussian latent variable model $p(x) = \sum_z p(x, z)$ used for clustering.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/7 Probabilistic PCA"><h2 class="text--truncate cardTitle_rnsV" title="Probabilistic Principal Component Analysis">ğŸ“„ï¸<!-- --> <!-- -->Probabilistic Principal Component Analysis</h2><p class="text--truncate cardDescription_PWke" title="Sometimes data is very high dimensional, its important features can be accurately captured in a low dimensional subspace. That is the purpose we use PCA.">Sometimes data is very high dimensional, its important features can be accurately captured in a low dimensional subspace. That is the purpose we use PCA.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/8 Bayesian Linear Regression"><h2 class="text--truncate cardTitle_rnsV" title="Bayesian Linear Regression">ğŸ“„ï¸<!-- --> <!-- -->Bayesian Linear Regression</h2><p class="text--truncate cardDescription_PWke" title="BLR is used when Gaussian discriminative model $p(y|X)$ used for regression with a Bayesian analysis for the weights.">BLR is used when Gaussian discriminative model $p(y|X)$ used for regression with a Bayesian analysis for the weights.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/9 Kernal Method"><h2 class="text--truncate cardTitle_rnsV" title="Kernal Method">ğŸ“„ï¸<!-- --> <!-- -->Kernal Method</h2><p class="text--truncate cardDescription_PWke" title="Define $\psi(x): \R^D \to \R^M$ and input data $X \in \R^{N \times D}$ and $\Psi \in \R^{N \times M}$, where $\Psi = \psi(X)$. Then we have the prediction matrix $\hat = \Psi w$. Then $y|x \sim N(w^T\psi(x), \sigma^2)$.">Define $\psi(x): \R^D \to \R^M$ and input data $X \in \R^{N \times D}$ and $\Psi \in \R^{N \times M}$, where $\Psi = \psi(X)$. Then we have the prediction matrix $\hat = \Psi w$. Then $y|x \sim N(w^T\psi(x), \sigma^2)$.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Basic Information to Multivariate Data"><h2 class="text--truncate cardTitle_rnsV" title="Basic Information to Multivariate Data">ğŸ“„ï¸<!-- --> <!-- -->Basic Information to Multivariate Data</h2><p class="text--truncate cardDescription_PWke" title="For a multivariate data, denote it with $p$ variables where $p \ge 2$, and with $n$ observations(item/experimental unit). We also denote as $x_$ where the measurement of kth variable on the jth item or experimental unit.">For a multivariate data, denote it with $p$ variables where $p \ge 2$, and with $n$ observations(item/experimental unit). We also denote as $x_$ where the measurement of kth variable on the jth item or experimental unit.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Beyond Linearity"><h2 class="text--truncate cardTitle_rnsV" title="Moving beyond Linearity">ğŸ“„ï¸<!-- --> <!-- -->Moving beyond Linearity</h2><p class="text--truncate cardDescription_PWke" title="We always make linear assumption to assume a model which make our life easizer. However, linear assumption is not always a good approximation, and sometimes even a poor one. That is, we extend the linear model by feature.">We always make linear assumption to assume a model which make our life easizer. However, linear assumption is not always a good approximation, and sometimes even a poor one. That is, we extend the linear model by feature.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Classfication"><h2 class="text--truncate cardTitle_rnsV" title="Classification">ğŸ“„ï¸<!-- --> <!-- -->Classification</h2><p class="text--truncate cardDescription_PWke" title="We may have a classification problem for a qualitative result from an unordered set $C$. The main goal of us is to:">We may have a classification problem for a qualitative result from an unordered set $C$. The main goal of us is to:</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Decision Tree"><h2 class="text--truncate cardTitle_rnsV" title="Decision Tree">ğŸ“„ï¸<!-- --> <!-- -->Decision Tree</h2><p class="text--truncate cardDescription_PWke" title="Decision Tree is a supervised learning algorithm that can be used for both classification and regression problems. It is a tree-like structure where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node holds a class label. The paths from root to leaf represent classification rules. And generally, a decision tree has a high variance and low bias.">Decision Tree is a supervised learning algorithm that can be used for both classification and regression problems. It is a tree-like structure where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node holds a class label. The paths from root to leaf represent classification rules. And generally, a decision tree has a high variance and low bias.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Discriminant Analysis"><h2 class="text--truncate cardTitle_rnsV" title="Discriminant Analysis">ğŸ“„ï¸<!-- --> <!-- -->Discriminant Analysis</h2><p class="text--truncate cardDescription_PWke" title="Discriminant Anaylsis parametrizes the distribution of $X | Y = 1$ and $X | Y = 0$.">Discriminant Anaylsis parametrizes the distribution of $X | Y = 1$ and $X | Y = 0$.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Fitted Model Measurement"><h2 class="text--truncate cardTitle_rnsV" title="Fitted Model Measurement">ğŸ“„ï¸<!-- --> <!-- -->Fitted Model Measurement</h2><p class="text--truncate cardDescription_PWke" title="To measure the fit of a model, we need to compare the model&#x27;s prediction with the actual data. The most common way to do this is to use the mean squared error (MSE) or the root mean squared error (RMSE). Depend on the different type of data, we can use different measurement to fit.">To measure the fit of a model, we need to compare the model&#x27;s prediction with the actual data. The most common way to do this is to use the mean squared error (MSE) or the root mean squared error (RMSE). Depend on the different type of data, we can use different measurement to fit.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Gradient Descent"><h2 class="text--truncate cardTitle_rnsV" title="Gradient Descent">ğŸ“„ï¸<!-- --> <!-- -->Gradient Descent</h2><p class="text--truncate cardDescription_PWke" title="Gradient descent is a optimization iterative algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent is to do derivatives to find the critical point. It asks a convex function to execut. That is, we can use grandient descent to find the minimum of a MSE.">Gradient descent is a optimization iterative algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent is to do derivatives to find the critical point. It asks a convex function to execut. That is, we can use grandient descent to find the minimum of a MSE.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Lasso Regression"><h2 class="text--truncate cardTitle_rnsV" title="Lasso Regression">ğŸ“„ï¸<!-- --> <!-- -->Lasso Regression</h2><p class="text--truncate cardDescription_PWke" title="The key point of Lasso Regression is to shrinks the coefficients toward 0 by penalizing their absolute values whereas find a model minimize $[\sum{i=1}^n (yi - \beta0 - \sum{j = 1}^p\betaj x)^2] + \lambda\sum{j = 1}^p|\betaj|= RSS + \lambda\sum{j = 1}^p|\betaj|$">The key point of Lasso Regression is to shrinks the coefficients toward 0 by penalizing their absolute values whereas find a model minimize $[\sum{i=1}^n (yi - \beta0 - \sum{j = 1}^p\betaj x)^2] + \lambda\sum{j = 1}^p|\betaj|= RSS + \lambda\sum{j = 1}^p|\betaj|$</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Logistic Regression"><h2 class="text--truncate cardTitle_rnsV" title="Logistic Regression">ğŸ“„ï¸<!-- --> <!-- -->Logistic Regression</h2><p class="text--truncate cardDescription_PWke" title="Logistic regression is a parametric approach to classification. It gives a structure of probability of $x$ by $p(X) =\frac{e^{\beta0 + \beta X}}{1+e^{\beta0 + \beta X}}$ where $\beta0$ is the intercept and $\beta$ is the coefficient matrix. Where $\frac{p(X)}{1-p(X)} = e^{\beta0 + \beta X}$ is the odds.">Logistic regression is a parametric approach to classification. It gives a structure of probability of $x$ by $p(X) =\frac{e^{\beta0 + \beta X}}{1+e^{\beta0 + \beta X}}$ where $\beta0$ is the intercept and $\beta$ is the coefficient matrix. Where $\frac{p(X)}{1-p(X)} = e^{\beta0 + \beta X}$ is the odds.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning"><h2 class="text--truncate cardTitle_rnsV" title="Machine Learning">ğŸ“„ï¸<!-- --> <!-- -->Machine Learning</h2><p class="text--truncate cardDescription_PWke" title="Machine learning is a subset of artificial intelligence, it is the study of computer algorithms that improve automatically through experience. Machine learning algorithms build a mathematical model based on sample data, known as &quot;training data&quot;, in order to make predictions or decisions without being explicitly programmed to perform the task.">Machine learning is a subset of artificial intelligence, it is the study of computer algorithms that improve automatically through experience. Machine learning algorithms build a mathematical model based on sample data, known as &quot;training data&quot;, in order to make predictions or decisions without being explicitly programmed to perform the task.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Model Selection"><h2 class="text--truncate cardTitle_rnsV" title="Model Selection">ğŸ“„ï¸<!-- --> <!-- -->Model Selection</h2><p class="text--truncate cardDescription_PWke" title="The way we always use is Subset Selection. We identify a subset of the $p$ predictors that we believe to be related to the response.">The way we always use is Subset Selection. We identify a subset of the $p$ predictors that we believe to be related to the response.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Multivariate Data Analysis among Machine Learning"><h2 class="text--truncate cardTitle_rnsV" title="Multivariate Data Analysis among Machine Learning">ğŸ“„ï¸<!-- --> <!-- -->Multivariate Data Analysis among Machine Learning</h2><p class="text--truncate cardDescription_PWke" title="Linear Regression">Linear Regression</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Multivariate Normal Distribution"><h2 class="text--truncate cardTitle_rnsV" title="Multivariate Normal Distribution">ğŸ“„ï¸<!-- --> <!-- -->Multivariate Normal Distribution</h2><p class="text--truncate cardDescription_PWke" title="Multivariate Normal Distribution is a generalization of the normal distribution to multiple dimensions. It is often a good approximation to the true distribution where by Central Limit Theorem, multivariate normal distribution is the sample distribution of many multivariate random variables.">Multivariate Normal Distribution is a generalization of the normal distribution to multiple dimensions. It is often a good approximation to the true distribution where by Central Limit Theorem, multivariate normal distribution is the sample distribution of many multivariate random variables.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Recap"><h2 class="text--truncate cardTitle_rnsV" title="Some recap of previous courses">ğŸ“„ï¸<!-- --> <!-- -->Some recap of previous courses</h2><p class="text--truncate cardDescription_PWke" title="Sufficient Statistics">Sufficient Statistics</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Ridge Regression"><h2 class="text--truncate cardTitle_rnsV" title="Ridge Regression">ğŸ“„ï¸<!-- --> <!-- -->Ridge Regression</h2><p class="text--truncate cardDescription_PWke" title="The key point of Ridge Regression is to find a model minimize $[\sum{i=1}^n (yi - \beta0 - \sum{j = 1}^p\betaj x)^2] + \lambda\sum{j = 1}^p\betaj^2= RSS + \lambda\sum{j = 1}^p\betaj^2$ where shrinks the coefficients toward 0">The key point of Ridge Regression is to find a model minimize $[\sum{i=1}^n (yi - \beta0 - \sum{j = 1}^p\betaj x)^2] + \lambda\sum{j = 1}^p\betaj^2= RSS + \lambda\sum{j = 1}^p\betaj^2$ where shrinks the coefficients toward 0</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Situation without the Test Data to do Model Validation"><h2 class="text--truncate cardTitle_rnsV" title="Situation without the Test Data to do Model Validation">ğŸ“„ï¸<!-- --> <!-- -->Situation without the Test Data to do Model Validation</h2><p class="text--truncate cardDescription_PWke" title="There are two common approaches for model selection when we don&#x27;t have $D_$">There are two common approaches for model selection when we don&#x27;t have $D_$</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Support Vector Machine"><h2 class="text--truncate cardTitle_rnsV" title="Support Vector Machine">ğŸ“„ï¸<!-- --> <!-- -->Support Vector Machine</h2><p class="text--truncate cardDescription_PWke" title="The trainning pointes with equality constraints $yi(xi^Tw + b) \ge M$ are called support vectors. We have Support Vector Machine (SVM) which is a classifier that finds the optimal hyperplane that separates the classes. SVM-like algorithms are often called max-margin or large-margin. Since the Primal-formulation is convex specially is a quadratic program. We can use SGD/GD to solve it. And its more common to solved by dual formulation.">The trainning pointes with equality constraints $yi(xi^Tw + b) \ge M$ are called support vectors. We have Support Vector Machine (SVM) which is a classifier that finds the optimal hyperplane that separates the classes. SVM-like algorithms are often called max-margin or large-margin. Since the Primal-formulation is convex specially is a quadratic program. We can use SGD/GD to solve it. And its more common to solved by dual formulation.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Tree Improving"><h2 class="text--truncate cardTitle_rnsV" title="Tree Improving">ğŸ“„ï¸<!-- --> <!-- -->Tree Improving</h2><p class="text--truncate cardDescription_PWke" title="We have many ways to improve the tree, like pruning, boosting, bagging, etc. In this section, we will introduce the bagging, random forest, and boosting.">We have many ways to improve the tree, like pruning, boosting, bagging, etc. In this section, we will introduce the bagging, random forest, and boosting.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/Unsupervised Model"><h2 class="text--truncate cardTitle_rnsV" title="Some Unsupervised Learning Model">ğŸ“„ï¸<!-- --> <!-- -->Some Unsupervised Learning Model</h2><p class="text--truncate cardDescription_PWke" title="Unsupervised learning is a the study without labels, normally, is the task of grouping, explaining and finding structured data.">Unsupervised learning is a the study without labels, normally, is the task of grouping, explaining and finding structured data.</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/sta414"><h2 class="text--truncate cardTitle_rnsV" title="STA414 Statistical Methods for Machine Learning II">ğŸ“„ï¸<!-- --> <!-- -->STA414 Statistical Methods for Machine Learning II</h2><p class="text--truncate cardDescription_PWke" title="Instructor: Piotr Zwiernik, Murat A. Erdogdu">Instructor: Piotr Zwiernik, Murat A. Erdogdu</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/Machine Learning/sta437"><h2 class="text--truncate cardTitle_rnsV" title="STA437 Method for Multivariate Data Analysis">ğŸ“„ï¸<!-- --> <!-- -->STA437 Method for Multivariate Data Analysis</h2><p class="text--truncate cardDescription_PWke" title="Instructor: Mehdi Molkaraie">Instructor: Mehdi Molkaraie</p></a></article></section></article><footer class="margin-top--lg"><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Deep Learning/csc413"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">CSC413 Neural Networks and Deep Learning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Machine Learning/0 Bayes Nets"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Conditional Independence and Bayes Nets</div></a></nav></footer></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Notes</a></li></ul></div><div class="col footer__col"><div class="footer__title">Yorafa&#x27;s Projects</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://coder.xfttech.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">UTLeetcoder<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://utodo.yorafa.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">UTodo<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://huxulm.github.io/lc-rating/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LC-Rating<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://home.yorafa.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Yorafa&#x27;s Home<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://yorafa.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Yorafa&#x27;s Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/Yorafa" target="_blank" rel="noopener noreferrer" class="footer__link-item">Yorafa&#x27;s Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2024 Yorafa. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>